{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing of the data, my main objective was to simplify the input and change the labels to match indices of output predictions (0-8) for each category of possible product. Since I had noticed, by printing some of the input image tensors, that all the points were either 0 or 255, I had thuis simplified the input images into 1 or 0 input. Beyond that I had also used unsqueez on the input to make the input chanels match the right dimensions for the later training of the model. I had also originally transfered my tensors to the gpu but had memory errors later on and had to revert back. (This error had caused my late submission to the leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8162fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11000, 64, 64])\n",
      "torch.Size([11000])\n",
      "True\n",
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data.dataloader\n",
    "import torch.utils.data.dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "#Since only black and white (not grayscale) turn all non 0 to 1 for easier computation\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "z = np.where(x>0,1,x)\n",
    "\n",
    "v = np.where(y==1,0,y)\n",
    "v = np.where(v==2,1,v)\n",
    "v = np.where(v==3,2,v)\n",
    "v = np.where(v==4,3,v)\n",
    "v = np.where(v==6,4,v)\n",
    "v = np.where(v==8,5,v)\n",
    "v = np.where(v==9,6,v)\n",
    "v = np.where(v==12,7,v)\n",
    "v = np.where(v==16,8,v)\n",
    "\n",
    "x_tr = torch.from_numpy(z)\n",
    "y_tr = torch.from_numpy(v)\n",
    "\n",
    "x_tr = x_tr.to(torch.float)\n",
    "y_tr = y_tr.to(torch.long)\n",
    "\n",
    "\n",
    "print(x_tr.size())\n",
    "print(y_tr.size())\n",
    "\n",
    "# indeces = torch.randperm(x_tr.size()[0])\n",
    "# x_tr = x_tr[indeces]\n",
    "# y_tr = y_tr[indeces]\n",
    "\n",
    "x_tr = x_tr.unsqueeze(1)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "x_tr = x_tr\n",
    "y_tr = y_tr\n",
    "\n",
    "print(x_tr.device)\n",
    "print(y_tr.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the architecture of my model, I had taken inspiration from both AlexNet and VGG to create a sort of combination of the two. I had experimented with different number of layers but found the best results to be with VGG type filters for the convolutional layers, a constant 3x3 with padding 1, non-changing output size within each convolutional layer similar to AlexNet, and a final choice of 3 Linear layers with descending output sizes. I also added 2 layers of dropout since I had initially noticed that my model was overfitting the data. I had added these after initially adding L2 regularization, since by itself the regularization was either too much or not enough regularization. For the optimization step, I had first chosen to do batch sizes of 50 since it was initially close to the limit of what my gpu could handle, and with higher batch sizes models produce better gradient estimates, thus having a better prediction. I had also shuffled the data before training as a general rule of thumb. After some initial research I had found that most, and the best, CNNs used cross entropy loss as the loss function so I had chosen to do the same. Finally for the actual optimizer, the choice of hyper-parameters was from experimenting, which was initally easy on the gpu. I found that with a learning rate of 0.001, the model was optimal. Although a lower learning rate and a higher number of epochs would eventually get me closer to the optimum, this took too long to train, and with a learning rate that was higher my model was overshooting the optimum and combined with momentum even started moving away from the optimum. The weight_decay hyper-parameter was used for L2 regularization and I simply chose the value through experimenting. Beyond the actual implementation,, I had also tried to add learning rate decay but found that training then became very slow so opted out from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01016d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.5147457122803\n",
      "470.3300688266754\n",
      "469.7131927013397\n",
      "469.22398042678833\n",
      "468.56616854667664\n",
      "467.03728556632996\n",
      "461.92041981220245\n",
      "426.99839997291565\n",
      "363.41094648838043\n",
      "336.8827098608017\n",
      "319.77425813674927\n",
      "299.24364817142487\n",
      "281.4598785638809\n",
      "262.47520637512207\n",
      "245.47710764408112\n",
      "231.4004549384117\n",
      "217.5665807723999\n",
      "205.7385209798813\n",
      "196.27826726436615\n",
      "186.44239872694016\n",
      "177.18075597286224\n",
      "168.48542866110802\n",
      "156.2542008459568\n",
      "150.64380049705505\n",
      "141.8523726463318\n",
      "132.85521295666695\n",
      "125.4924908876419\n",
      "122.7407845556736\n",
      "120.09815749526024\n",
      "111.03152757883072\n",
      "103.53109304606915\n",
      "101.5867197662592\n",
      "99.68605595827103\n",
      "90.78144611418247\n",
      "86.97434209287167\n",
      "80.50772165507078\n",
      "79.69040144979954\n",
      "72.72028091549873\n",
      "68.64226211607456\n",
      "64.77623284608126\n",
      "59.540625140070915\n",
      "56.92116494476795\n",
      "55.476712457835674\n",
      "52.21532768011093\n",
      "47.429237212985754\n",
      "46.29065925627947\n",
      "42.00908836722374\n",
      "38.04845245182514\n",
      "35.445669235661626\n",
      "31.523236643522978\n",
      "31.34634944051504\n",
      "28.901652980595827\n",
      "26.173120699822903\n",
      "23.454110626131296\n",
      "23.091683942824602\n",
      "21.249567209742963\n",
      "19.57338322326541\n",
      "18.93856598995626\n",
      "17.264685148373246\n",
      "15.395441701635718\n",
      "16.260580367408693\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,64,3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(64,128,3,padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128,256,3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(256,256,3,padding=1)\n",
    "        #14\n",
    "\n",
    "        # self.conv5 = nn.Conv2d(64,128,3,padding=1)\n",
    "        # self.conv6 = nn.Conv2d(128,128,3,padding=1)\n",
    "        # self.conv7 = nn.Conv2d(128,128,3,padding=1)\n",
    "\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        self.f1 = nn.Linear(16384,8192)\n",
    "        self.f2 = nn.Linear(8192,4096)\n",
    "        self.f3 = nn.Linear(4096,9)\n",
    "        self.soft = nn.Softmax()\n",
    "        # self.f4 = nn.Linear(380,9)\n",
    "        # self.f5 = nn.Linear(120,9)\n",
    "        # self.f6 = nn.Linear(100,9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        #x = self.drop1(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv4(x)),2)\n",
    "\n",
    "        # x = F.relu(self.conv5(x))\n",
    "        # x = F.relu(self.conv6(x))\n",
    "        # x = F.max_pool2d(F.relu(self.conv7(x)),2)\n",
    "\n",
    "        # x = F.relu(self.conv8(x))\n",
    "        # x = F.relu(self.conv9(x))\n",
    "        # x = F.max_pool2d(F.relu(self.conv10(x)),2)\n",
    "\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.f1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.f2(x))\n",
    "        # x = F.relu(self.f3(x))\n",
    "        # x = F.relu(self.f4(x))\n",
    "        # x = F.relu(self.f5(x))\n",
    "        x = self.f3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Net()\n",
    "\n",
    "data = torch.utils.data.TensorDataset(x_tr,y_tr)\n",
    "batch = 50\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(data, batch,True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001,momentum=0.9, weight_decay=0.005)\n",
    "##optimizer = optim.Adam(net.parameters(),lr=0.001,betas=(0.9,0.999),weight_decay=0.005)\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.01,total_iters=10)\n",
    "\n",
    "prior_loss = 2000\n",
    "for epoch in range(100):\n",
    "\n",
    "    \n",
    "    running_accuracy = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(inputs)\n",
    "\n",
    "        correct = torch.sum(labels == torch.argmax(output, dim=1)).item()\n",
    "        running_accuracy += correct / batch\n",
    "\n",
    "        loss = criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(running_loss)\n",
    "    if(prior_loss - running_loss < 0.05): break\n",
    "    prior_loss = running_loss \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4500, 64, 64])\n",
      "torch.Size([4500, 1])\n",
      "Test accuracy: 83.17777777777778 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "z = np.where(x_test_half>0,1,0)\n",
    "\n",
    "y_tt = np.where(y_test_half==1,0,y_test_half)\n",
    "y_tt = np.where(y_tt==2,1,y_tt)\n",
    "y_tt = np.where(y_tt==3,2,y_tt)\n",
    "y_tt = np.where(y_tt==4,3,y_tt)\n",
    "y_tt = np.where(y_tt==6,4,y_tt)\n",
    "y_tt = np.where(y_tt==8,5,y_tt)\n",
    "y_tt = np.where(y_tt==9,6,y_tt)\n",
    "y_tt = np.where(y_tt==12,7,y_tt)\n",
    "y_tt = np.where(y_tt==16,8,y_tt)\n",
    "\n",
    "x_test_half = torch.from_numpy(z)\n",
    "y_test_half = torch.from_numpy(y_tt)\n",
    "\n",
    "print(x_test_half.size())\n",
    "print(y_test_half.size())\n",
    "\n",
    "x_test_half = x_test_half.to(torch.float)\n",
    "\n",
    "x_test_half = x_test_half.unsqueeze(1)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(x_test_half,y_test_half)\n",
    "testloader = torch.utils.data.DataLoader(test_data)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        # print(torch.argmax(outputs,dim=1))\n",
    "        # print(labels.item())\n",
    "        correct += torch.sum(labels == torch.argmax(outputs, dim=1)).item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_accuracy = 100 * (correct/total)\n",
    "print(\"Test accuracy: {} %\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
